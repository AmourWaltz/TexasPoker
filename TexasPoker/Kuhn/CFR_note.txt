遗憾值：已采取动作a，没有采取动作b的遗憾值为，采取动作b的收益值-采取动作a的收益值：V(a)-V(b)，玩家没有采取某一策略的收益-实际采取策略的收益。
遗憾值匹配，对正遗憾值进行归一化，即指导下一步应该采取策略的概率，把每局中每个策略的遗憾值与上一局该策略的遗憾值进行累加并存储。
在扩展式博弈中，遗憾匹配需要对所有的纯策略的遗憾值进行存储，纯策略在信息集与可采用的策略呈现指数分布，因此即使在中型的扩展式游戏中也是不可行的。因为在每一次迭代中更新策略文件的时间是呈指数分布的，同时对存储的要求也是呈指数增长的。每次迭代递归的访问信息集，根据遗憾匹配等式计算混合策略。在一般情况下，我们认为虚拟遗憾的计算就是指故意的使 player i 的策略被限定在信息集 Ii，换一种说法就是，排除了真实的通过计算得来的 player i 的数据的可能性。CFR 算法通过多次迭代，将整体遗憾分解到各个独立的信息集中计算局部最小遗憾值。
Z代表博弈树中所有的叶子节点，h代表博弈树中的非叶子节点。μi(z)表示玩家i在叶子节点z的效用值。定义在非叶子节点h处虚拟价值：选择h的概率和与从h到z的概率与叶子节点z的收益值之积。再计算在节点h不采取动作a的虚拟遗憾值，然后计算在信息集I中不采取a的虚拟遗憾值。在多次迭代中累积虚拟遗憾值，最后通过后悔匹配得到当前的策略值。
CFR 算法通过一次迭代在被访问到的每个信息集上计算虚拟价值和虚拟遗憾值，当下一次迭代开始时，将会带入上次迭代的结果通过遗憾匹配计算新的博弈策略。CFR 算法不断的迭代执行，递归的回溯博弈树进行计算并更新策略，从而最小化玩家的平均遗憾值。
